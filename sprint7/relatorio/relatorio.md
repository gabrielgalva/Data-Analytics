<h1>- Sprint 7:</h1>
- Durante a sétima sprint, dediquei-me aos cursos de Spark com Pyspark e Learn By Example: Hadoop, MapReduce for Big Data problems. Dos dias 04/09 até o dia 18/09, abaixo relatei o que explorei durante esses cursos. 

<h2>•  Learn By Example: Hadoop, MapReduce for Big Data problems</h2>

<h2>• Seção 1:Introduction</h2>
- Uma breve introdução ao curso e ao que será abordado.


<h2>• Seção 2:Why is Big Data a Big Deal</h2>
- Aprofundei-me no paradigma do Big Data. Fiquei impressionado com a quantidade de dados gerados hoje em dia e como o Hadoop desempenha um papel crucial na sua gestão e análise.


<h2>• Seção 3:Installing Hadoop in a Local Environment</h2>
-Nesta parte do curso, eu explorei a instalação do Hadoop em meu ambiente local. Aprendi sobre dois modos de instalação:

Instalação em Modo Standalone do Hadoop: Esta configuração é ideal para testes e desenvolvimento em uma única máquina.

Instalação em Modo Pseudo-Distribuído do Hadoop: Essa opção simula um ambiente de cluster em minha máquina local, permitindo testar cenários mais complexos.


<h2>• Seção 4:The MapReduce "Hello Word"</h2>
- Comecei com um exemplo prático "Hello World" para entender como o MapReduce funciona na prática.Explorei a filosofia subjacente ao MapReduce, compreendendo sua abordagem simplificada para o processamento paralelo de dados.Visualizei o processo de MapReduce, desde o mapeamento até a redução, entendendo como os dados são processados em etapas.Aprofundei em cada estágio do processamento MapReduce, explorando os detalhes e a lógica por trás de cada passo.Entendi como o Reducer agrega e consolida os resultados intermediários gerados pelos Mappers.Por fim, explorei o conceito de "Job" no contexto do Hadoop e do MapReduce, entendendo como ele coordena as tarefas MapReduc


<h2>• Spark com PySpark</h2>

<h2>• Seção 1:Introdução</h2>
- Na primeira parte do curso, fui introduzido ao tema geral e ao que eu poderia esperar aprender. Também obtive acesso ao material de apoio necessário para as lições. Aprendi os conceitos introdutórios do Spark e sua importância no processamento de big data. Explorando a arquitetura e os componentes do Apache Spark, ganhei uma compreensão mais profunda de seu funcionamento interno. 


<h2>• Seção 2:Instalação e Primeiros Passos</h2>
- Comecei com atenção aos requisitos do ambiente necessário para o curso. Em seguida, segui as instruções para baixar os recursos essenciais. Configurei uma máquina virtual Ubuntu, instalei o Apache Spark e as bibliotecas adicionais necessárias. Realizei exemplos práticos e baixei dados de exemplo para atividades futuras. 


<h2>• Seção 3:DataFrames e RDDs</h2>
-  Entendi o processamento de dados com RDD, Dataset e Dataframe. Realizei operações em RDDs, compreendi os conceitos por trás de DataFrames e como eles podem ser manipulados. Implementei principais ações e transformações e aprendi a exportar e importar dados no ambiente Spark. Enfrentei desafios práticos propostos pelo curso, analisando em detalhes as soluções apresentadas.


<h2>• Seção 4:Spark SQL</h2>
- Aprofundei meu conhecimento em Spark SQL, executando consultas em dados e gerenciando bancos de dados e tabelas. Aprendi a criar views e compreendi as diferenças entre DataFrames e tabelas SQL tradicionais. Realizei operações de junção de dados e explorei o uso de Spark SQL em consultas avançadas. Mais uma vez, enfrenteio um desafio prático e analisei as soluções propostas.


<h2>• Seção 6:Criando Aplicações</h2>
- Desenvolvi aplicações práticas usando o Spark. Implementei uma aplicação que escrevia no console e outra que aceitava parâmetros de entrada. Também explorei opções e argumentos de linha de comando. Desenvolvi uma aplicação de conversão de formatos de arquivos usando o Spark.


<h2>• Seção 9:Otimização</h2>
- Aprendi a otimizar o desempenho do Spark através do particionamento de dados e técnicas de bucketing. Compreendi o conceito de cache e sua aplicação no Spark, bem como a persistência de dados para melhor gerenciamento.


<h2>• Seção 10:Outros Aspectos</h2>
- Explorando tópicos avançados, usei o Spark com Notebooks Jupyter e converti dados do Pandas em DataFrames do Spark. Utilizei a interface do usuário do Spark para monitoramento e depuração e conheci as configurações e ajustes disponíveis na plataforma



