
<h1>- Sprint 2:</h1>
<h4>– Neste documento, apresento um resumo da Sprint 2, que abrangeu dois tópicos: SQL para Análise de Dados, do básico ao avançado, e Big Data Fundamentos 3.0. A Sprint ocorreu no período de 26/06 a 07/07 e organizei em seções distintas, cada uma correspondendo a um assunto específico. </h4>

<h2>• SQL para Ánalise de Dados do básico ao avançando:</h2>

<h3>- Seção 1:</h3>
<h4>- Na primeira seção do curso "SQL para Análise de Dados do básico ao avançado", tive o acesso aos arquivos do curso em anexo disponibilizado na aula, esse recurso permitiu baixar todos os materiais necessários para acompanhar as lições e praticar.</h4>

 
<h3>- Seção 2:</h3>
<h4>- Nessa parte do curso, configurei o ambiente de trabalho para trabalhar com SQL e análise de dados, isso incluiu a configuração da minha máquina,  tive uma visão geral do pgAdmin e a configuração de um banco de dados, também recebi material de apoio, como documentação e exemplos práticos. </h4>

 
<h3>- Seção 3:</h3>
<h4>- Aprofundei nos comandos básicos, como SELECT, DISTINCT, WHERE, ORDER BY e LIMIT. Aprendi a escrever consultas eficientes, filtrar resultados, ordenar dados e controlar o número de registros retornados. Além disso, enfrentei um desafio prático que me permitiu aplicar o conhecimento adquirido.</h4>

<h3>- Seção 4:</h3>
<h4>- Explorei os operadores no SQL, aprendi sobre os operadores aritméticos, que permitem realizar cálculos matemáticos, como adição e subtração, diretamente nas consultas. Também estudei os operadores de comparação, que são usados para comparar valores e filtrar os resultados com base em condições específicas. Além disso, abordei os operadores lógicos, como AND e OR, que são úteis para combinar condições em consultas mais complexas e no fim fiz um desafio prático.</h4>


<h3>- Seção 5:</h3>
<h4>- Entendi as funções de agregação no SQL, aprendi a utilizar funções como  COUNT, MIN e MAX para realizar cálculos em conjuntos de dados. Também estudei o comando GROUP BY, que me permitiu agrupar os resultados da consulta com base em colunas específicas, além disso, aprendi sobre o comando HAVING, que usei para filtrar os resultados após a aplicação do GROUP BY.</h4>


<h3>- Seção 6:</h3>
<h4>- Obtive conhecimentos sobre os comandos de JOIN no SQL, aprendi sobre os diferentes tipos de Join, como INNER JOIN, LEFT JOIN, RIGHT JOIN e FULL JOIN, e como eles podem ser usados para combinar dados de várias tabelas. Analisei exemplos práticos de como aplicar esses comandos e fiz varias vezes a atividade que está disponível sobre o uso dos Joins. 


<h3>- Seção 7:</h3>
<h4>- Aprendi sobre os comandos de UNION no SQL, descobri como utilizar o UNION para combinar resultados de consultas, o UNION ALL para incluir duplicatas, acompanhei e fiz os exemplos práticos para entender como aplicar esses comandos.</h4>


<h3>- Seção 8:</h3>
<h4>- Estudei sobre subqueries, os tipos e aplicação dessas ferramentas em consultas, com exemplos práticos e um desafio, aprendi que as subqueries podem ser correlacionadas, dependendo dos resultados da consulta principal, ou não correlacionadas.</h4>

<h3>- Seção 9:</h3>
<h4>- Na aula de tratamento de dados, aprendi comandos básicos, conversão de unidades, limpeza e normalização de dados, manipulação de texto, tratamento de datas e uso de funções. </h4>

<h3>- Seção 10:</h3>
<h4>- Sobre manipulação de tabelas, tive a oportunidade de aprender habilidades fundamentais para lidar com dados em um banco de dados. Aprendi como criar tabelas, definindo os nomes das colunas e os tipos de dados adequados, compreendi a importância de aplicar restrições, como chaves primárias e estrangeiras, para garantir a integridade dos dados.</h4>

<h3>- Seção 11:</h3>
<h4>- A aula abordou a criação de um dashboard para acompanhar as vendas de uma empresa, foram apresentadas queries SQL para extrair informações relevantes dos dados de vendas, como total de vendas por período, produtos mais vendidos e clientes mais frequentes. Também foi ensinado como criar gráficos para visualizar os dados de forma mais intuitiva,o objetivo foi desenvolver um dashboard eficiente para análise e monitoramento das vendas.</h4>

<h3>- Seção 12:</h3>
<h4>- Foi a criação de um projeto que teve como objetivo ensinar como analisar o perfil dos clientes por meio de queries SQL e representar os resultados por meio de gráficos, isso possibilita uma compreensão mais clara e visual dos dados de perfil de cliente, auxiliando na identificação de padrões e insights relevantes para a empresa.</h4>

<h3>- Seção 13:</h3>
<h4>- E a ultima seção foi um encerrameto do curso ,no geral o curso me proporcionou uma base sólida em SQL, capacitando-me a realizar análises de dados mais sofisticadas e a obter insights relevantes para os meus projetos. Saio do curso motivado a explorar ainda mais as funcionalidades e recursos do SQL, e continuarei a aperfeiçoar minhas habilidades nessa área em busca de melhorias contínuas em minha prática profissional. </h4>

<h2>• Big Data Fundamentos 3.0:</h2>

<h3>- Seção 1:</h3>
<h4>- No inicio do curso de Big Data, fui introduzido ao programa e à plataforma de estudo, aprendi a navegar pela plataforma e conheci o conteúdo do curso. Também recebi informações sobre a avaliação final e a conclusão do curso, além disso, ganhei um eBook gratuito da Data Science Academy para complementar meus estudos.</h4>


<h3>- Seção 2:</h3>
<h4>- Aprendi sobre o que é Big Data, descobri alguns fatos interessantes sobre ele e entendi sua definição,também aprendi sobre os "4 Vs" do Big Data: volume, velocidade, variedade e veracidade. Vi alguns números impressionantes relacionados ao Big Data e entendi a diferença entre Big Data e Ciência de Dados, conheci exemplos de como o Big Data é aplicado em diferentes áreas, no final, fiz um questionário para testar meus conhecimentos.</h4>


<h3>- Seção 3:</h3>
<h4>- Abordei os sistemas de armazenamento de dados no contexto do Big Data, compreendi a diferença entre bancos de dados relacionais e NoSQL, explorando suas respectivas adequações. Também familiarizei-me com o conceito de data warehouse, um sistema centralizado para armazenar e analisar dados provenientes de várias fontes.
Além disso, descobri os data lakes, que são repositórios flexíveis para armazenamento de dados brutos, e suas vantagens,também adquiri conhecimento sobre data stores, soluções que permitem armazenamento rápido e escalável de dados, por fim explorei os sistemas híbridos de armazenamento, que combinam diversas tecnologias para atender às demandas específicas do Big Data.</h4>


<h3>- Seção 4:</h3>
<h4>- Mergulhei no mundo do armazenamento e processamento paralelo no contexto do Big Data, explorando o tema, descobri o poder dos clusters de computadores interconectados, capazes de lidar com grandes volumes de dados de forma eficiente, compreendi a importância do armazenamento paralelo, com uma abordagem distribuída que permite acessar e armazenar dados de forma eficiente em um cluster.
Foi fascinante conhecer o Apache Hadoop, um software amplamente utilizado nesse cenário, ele oferece recursos para armazenamento paralelo e processamento distribuído de dados em grande escala. Através do processamento paralelo de Big Data, pude entender como operações simultâneas realizadas em um cluster podem acelerar significativamente o tempo de processamento.
Adentrando na arquitetura de armazenamento e processamento paralelo, ficou claro como diferentes tecnologias e componentes se unem para otimizar o desempenho no processamento de Big Data. Explorar soluções disponíveis no mercado para o armazenamento e processamento paralelo revelou a variedade de opções e abordagens disponíveis para enfrentar os desafios desse campo em constante evolução.</h4>


<h3>- Seção 5:</h3>
<h4>- Aprendi sobre cloud computing e sua relação com o big data, conheci os principais provedores em nuvem, com destaque para a Amazon Web Services (AWS). Também participei de uma demonstração prática de processamento de big data na nuvem usando AWS. </h4>


<h3>- Seção 6:</h3>
<h4>- Primeiramente, aprendi sobre Machine Learning (aprendizado de máquina) e entendi que se trata de algoritmos que permitem que os sistemas aprendam a partir dos dados.Em seguida, explorei o MLOps, que é a aplicação de práticas de engenharia de software ao desenvolvimento de modelos de Machine Learning, isso envolve automação, controle de versão, monitoramento e escalabilidade.Consegui compreender como o MLOps resolve desafios comuns na implementação de modelos de Machine Learning, como gerenciamento de versões e garantia de qualidade.E aprendi sobre o DataOps, que se refere a práticas e processos para melhorar a colaboração e eficiência na gestão de dados. </h4>


<h3>- Seção 7:</h3>
<h4>- Explorei o conceito de "Dados Como Serviço" (DaaS) e as arquiteturas modernas de Big Data, o DaaS  mostrou como é possível disponibilizar os dados como um serviço, permitindo um acesso flexível e escalável. Eu entendi a arquitetura por trás do DaaS e os benefícios que ele traz. Além disso, aprendi sobre abordagens inovadoras na arquitetura de Big Data, como o Data Lakehouse e o Data Mesh, que proporcionam maior flexibilidade no armazenamento, também fui apresentado a algumas soluções comerciais disponíveis para implementar o DaaS e as arquiteturas modernas de Big Data.</h4>


<h3>- Seção 8:</h3>
<h4>- Estudei sobre ETL (Extração, Transformação e Carga de Dados), compreendi sua importância no gerenciamento de dados, envolvendo a extração, transformação e carga em um destino final. Também conheci as principais soluções disponíveis no mercado e participei de uma demonstração prática envolvendo ETL e Big Data.</h4>


<h3>- Seção 9:</h3>
<h4>- Entendi a importância do Big Data Analytics e como as empresas estão utilizando os dados para impulsionar seus negócios, conheci casos de uso do Big Data Analytics e aprendi os passos essenciais para começar um projeto de Big Data, como definir um Business Case sólido, planejar o projeto, estabelecer requisitos técnicos e avaliar o valor comercial do projeto.</h4>


<h3>- Seção 10:</h3>
<h4>-  Nessa parte final do curso, finalizei a avaliação e obtive meu certificado de conclusão com um resultado de 82% de aproveitamento, comprovando meu conhecimento em Big Data.</h4>
